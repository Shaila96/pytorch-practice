{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN is an extension of FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch            ## for creating tensors\n",
    "import torch.nn as nn   ## for modeling\n",
    "import torchvision.transforms as transforms    ## make data iterable\n",
    "import torchvision.datasets as dsets           ## make data iterable\n",
    "\n",
    "\n",
    "from torch.autograd import Variable  ## make variable for enable gradients\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Steps:\n",
    "(Exactly same as Logistic Regression)\n",
    "- Step 1: Load dataset\n",
    "- Step 2: Make dataset iterable\n",
    "- Step 3: Create model class\n",
    "- Step 4: Instantiate model class\n",
    "- Step 5: Instantiate loss class\n",
    "- Step 6: Instantiate optimizer class\n",
    "- Step 7: Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dsets.MNIST(root='./data',\n",
    "                      train=True,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n",
    "\n",
    "test_df = dsets.MNIST(root='./data',\n",
    "                      train=False,\n",
    "                      transform=transforms.ToTensor(),\n",
    "                      download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "n_epochs = int(n_iters / (len(train_df) / batch_size))\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_df,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_df,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetModel(nn.Module):\n",
    "    # Here, hidden_size is extra then logistic regression\n",
    "    # It denotes how many non-linear dimension are there\n",
    "    def __init__(self, input_size, hidden_size, num_classes): \n",
    "        super(FeedForwardNeuralNetModel, self).__init__()\n",
    "        \n",
    "        # Linear function 1: 784 --> 100\n",
    "        # input to hidden\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        \n",
    "        ##################\n",
    "        # Non-linearity #\n",
    "        #################\n",
    "        \n",
    "        #----- Model A: 1 Hidden Layer FeedForward Neural Network (Sigmoid Activation) -----#\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #----- Model B: 1 Hidden Layer FeedForward Neural Network (Tanh Activation) -----#\n",
    "        #self.tanh = nn.Tanh()\n",
    "        \n",
    "        #----- Model C: 1 Hidden Layer FeedForward Neural Network (RELU Activation) -----#\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Linear function 2: 100 --> 100\n",
    "        # hidden to hidden\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "           \n",
    "        #----- Model D: 2 Hidden Layer FeedForward Neural Network (RELU Activation) -----#\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "         \n",
    "            \n",
    "        \n",
    "        # Linear function 3: 100 --> 100\n",
    "        # hidden to hidden\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "             \n",
    "        #----- Model E: 3 Hidden Layer FeedForward Neural Network (RELU Activation) -----#\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # Linear --> Non-linear --> Linear\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        \n",
    "        # Non-linearity 1\n",
    "        #out = self.sigmoid(out)\n",
    "        #out = self.tanh(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        \n",
    "        # Linear function 3\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 3\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Instantiate Model Class\n",
    "\n",
    "- **Input** dimension: 784\n",
    "    - Size of image\n",
    "    - 28*28=784\n",
    "- **Output** dimension: 10\n",
    "    - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "- **Hidden** dimension: 100\n",
    "    - Can be any number\n",
    "    - Similar term\n",
    "        - Number of neurons\n",
    "        - Number of non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100 ## extra than logistic regression\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedForwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Instantiate Loss Class\n",
    "- Feedforward Neural Network\n",
    "    - Cross Entropy Function\n",
    "- Logistic Regression\n",
    "    - Cross Entropy Function\n",
    "- Linear Regression\n",
    "    - Mean Squared Error (MSE) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What happens in nn.CrossEntropyLoss()\n",
    "- Computes softmax (logistic/softmax function)\n",
    "- Computes cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Instantiate Optimizer Class\n",
    "- parameters = parameters - learning_rate * parameters_gradients\n",
    "- **At every iteration, we update our model's parameters**\n",
    "\n",
    "\n",
    "###### What is the purpose fo the optimizer class\n",
    "- Update the model's parameter at every iteration\n",
    "- So that, we can get a better model to do the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters In-Depth\n",
    "- For more clarification watch the video no 29 (from 13min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x122c584f8>\n",
      "8\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "\n",
    "# Fully Connect (FC) 1 Parameters --> A1\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "\n",
    "# FC 1 Bias Parameters --> B1\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "\n",
    "# FC 2 Parameters --> A2\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "\n",
    "# FC 2 Bias Parameters --> B2\n",
    "print(list(model.parameters())[3].size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.2923882007598877. Accuracy: 9.\n",
      "Iteration: 1000. Loss: 2.298668622970581. Accuracy: 8.\n",
      "Iteration: 1500. Loss: 2.288738965988159. Accuracy: 10.\n",
      "Iteration: 2000. Loss: 2.2903330326080322. Accuracy: 16.\n",
      "Iteration: 2500. Loss: 2.28289794921875. Accuracy: 18.\n",
      "Iteration: 3000. Loss: 2.2831785678863525. Accuracy: 20.\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "\n",
    "for epoch in range(n_epochs): ## this loop will go through all 60,000 images 5 times\n",
    "    for idx, (images, labels) in enumerate(train_loader): ## this loop will go through all 60,000 images once\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameter\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calcultae Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Calculate gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter%500==0:\n",
    "            # Calculate accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # Iterate through the test dataset\n",
    "            for images, labels in test_loader:\n",
    "                \n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only the to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of lables\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}.'.format(iter, loss.data, accuracy))\n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning\n",
    "- 2 ways to expand a neural network\n",
    "    - More non-linear activation units/neurons/hidden dimension\n",
    "        - Here we put 100 hidden layer\n",
    "        - We can make it larger\n",
    "    - More hidden layer\n",
    "        - Here we did upto 3 hidden layer\n",
    "- Cons\n",
    "    - Need a larger dataset\n",
    "        - Curse od dimentionality\n",
    "    - Does not necessarily mean higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
